---
output: html_document
---
# Your Turn: Lab exercise


```{r}
library(tidyverse)
library(broom)
install.packages("remotes")
library(remotes)
remotes::install_github("njtierney/broomstick")
library(broomstick) 
library(rpart)
library(rpart.plot)
library(here)
```

- Return of the paintings data
- Just predict price with year


- If you were an auctioneer, and you were given a piece with the characteristics: 
  
How could you predict the log scaled price it would go for at auction?

The data set we are using is the paris paintings dataset we discussed
in the linear models lecture. The data dictionary is available here:
 http://www2.stat.duke.edu/~cr173/Sta112_Fa16/data/paris_paintings.html

```{r}
pp <- read_csv(here::here("data/paris-paintings.csv"), na = c("-", "n/a", "-", ""))
pp
```


## Reminder linear models

```{r}
pp_lm <- lm(logprice ~ Height_in + Width_in, data = pp)

pp_lm_aug <- augment(pp_lm)
```


## Using a regression tree


```{r rpart}
pp_rp <- rpart(logprice ~ Height_in + Width_in + year, data = pp)
pp_rp_aug <- augment(pp_rp)

rpart.plot(pp_rp)
```

### Your turn plot residuals vs fitted values for both models, how do they compare?

```{r, eval = FALSE}
pp_lm_aug %>% ggplot(aes(x = ---, y = ---)) +
   ---()

pp_rp_aug %>% ggplot(aes(x = ---, y = ---)) +
   ---()


# also, look at fitted vs predicted
pp_lm_aug %>% ggplot(aes(x = ---, y = logprice)) +
   ---()

pp_rp_aug %>% ggplot(aes(x = ---, y = logprice)) +
   ---()


```


### Figuring out what drives price

Describe why we've subsetted the data (to avoid perfect predictors?)


```{r}
pp_rp_all <- rpart(logprice ~ ., data = select(pp, 
                                               logprice,
                                               origin_author,
                                               school_pntg,
                                               artistliving,
                                               authorstyle,
                                               endbuyer,
                                               Height_in:Surface,
                                               mat,
                                               engraved:finished,
                                               relig,
                                               landsALL,
                                               arch:other))

rpart.plot(pp_rp_all, roundint = FALSE)

```

What do the residuals and goodness of fit look like?

How does variable importance work?

> After calculating all the potential splits, it gives a score to each variable for the number of times it was used in splitting

```{r}
tidy(pp_rp_all) %>% 
  ggplot(aes(x = importance,
             y = reorder(variable, importance))) + 
  geom_col()

```

### Price prediction 

Use the most 3 important variable to build a new tree:

```{r, eval = FALSE}

# use tidy to find top3 most important vars (yours might vary from the solution)
tidy(---)

pp_rp_simple <- rpart(logprice ~ --- + --- + --- , data = pp)


```

You are handed a new painting where the origin author and school of the painting are unknown and the material is cuivre (copper), `mat` = 'c'. What is the expected price?

```{r, eval = FALSE}
new_painting <- tibble(--- = "X", --- = "X", mat = "c")

augment(---, --- = new_painting)

# You could use this plot to predict too:
rpart.plot(---)

```

How does this compare to a prediction from a linear model? Which model would be considered 'better'? Can you improve it by modify the rpart controls?


```{r, eval = FALSE}
pp_lm_simple <- lm(--- ~ --- + --- + ---, data = pp)

augment(---, --- = new_painting)

```


